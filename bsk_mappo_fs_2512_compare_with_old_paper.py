import math
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import sys
import os
import Basilisk
import pandas as pd # Th√™m pandas ƒë·ªÉ l∆∞u log

# --- 1. BASILISK IMPORTS ---
try:
    from Basilisk.utilities import SimulationBaseClass, macros, simIncludeGravBody, orbitalMotion, vizSupport
    from Basilisk.simulation import spacecraft
    from Basilisk.simulation import extForceTorque # Ch·ªâ d√πng ExtForce, t·∫Øt Drag/Atmo ƒë·ªÉ an to√†n
    from Basilisk.architecture import messaging 
    from Basilisk.simulation import spiceInterface
    print("[INIT] Basilisk libraries loaded.")
except Exception as e:
    print("[ERROR] Basilisk not found. ", e)
    sys.exit(1)

# --- 2. MAPPO NETWORK ---
class MultiAgentActorCritic(nn.Module):
    def __init__(self, obs_dim, global_state_dim, action_dim, hidden=256):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, action_dim)
        )
        self.critic = nn.Sequential(
            nn.Linear(global_state_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, 1)
        )
    def act(self, obs): return self.actor(obs)
    def evaluate(self, global_state): return self.critic(global_state)

# --- 3. ENVIRONMENT (FINAL VERSION) ---
class BasiliskFullMissionEnv:
    def __init__(self, n_sats=4, n_targets=50, viz=False, hurricane_mode=True):
        self.n_sats = n_sats
        self.n_targets = n_targets
        self.viz = viz
        self.hurricane_mode = hurricane_mode # K√≠ch ho·∫°t ch·∫ø ƒë·ªô B√£o
        
        self.earth_radius = 6371e3
        self.mu = 398600.4418e9
        
        # --- 1. ƒê·ªíNG B·ªò THAM S·ªê V·ªöI B√ÄI B√ÅO (Agile EOS Model) ---
        # B√†i b√°o th∆∞·ªùng d√πng v·ªá tinh LEO 600-700km, xoay nhanh
        self.SAT_ALTITUDE = 600e3  # 600 km (Chu·∫©n LEO)
        self.SLEW_RATE_DEG = 10.0   # 10 ƒë·ªô/s (Agile Satellite standard)
        self.max_slew_rate = math.radians(self.SLEW_RATE_DEG)
        
        # NƒÉng l∆∞·ª£ng & B·ªô nh·ªõ (Gi·∫£ l·∫≠p EOS-1/Pleiades)
        self.fuel_max = 100.0
        self.batt_max = 100.0
        self.buffer_max = 100       # TƒÉng buffer l√™n ƒë·ªÉ ph√π h·ª£p mission ch·ª•p b√£o
        
        # C·∫•u h√¨nh Camera & Downlink
        self.CAPTURE_MAX_DIST = 1500e3    # Cho ph√©p ch·ª•p xa h∆°n ch√∫t
        self.CAPTURE_MAX_OFF_NADIR = 45.0 # G√≥c chu·∫©n ch·ª•p ·∫£nh v·ªá tinh (45 ƒë·ªô)
        
        # Tr·∫°m m·∫∑t ƒë·∫•t (Svalbard, Matera, McMurdo, v.v...) - Chu·∫©n ESA/NASA
        self.gs_coords = [
            (78.22, 15.40),   # Svalbard (Na Uy) - Tr·∫°m c·ª±c quan tr·ªçng nh·∫•t
            (40.65, 16.70),   # Matera (√ù)
            (21.02, 105.85),  # Hanoi (Vietnam)
            (-33.86, 151.20), # Sydney
            (40.71, -74.00)   # New York
        ]
        self.n_gs = len(self.gs_coords)
        self.global_captured_targets = set()
        
        # Action & Observation Space
        self.ORBIT_CHOICES = [10.0, 45.0, 97.0] # 97 l√† Sun-Synchronous Orbit (SSO)
        self.ACT_CHARGE = self.n_targets + self.n_gs
        self.ACT_ALT_UP = self.ACT_CHARGE + 1
        self.ACT_ALT_DOWN = self.ACT_CHARGE + 2
        self.ACT_GOTO_INC_START = self.ACT_ALT_DOWN + 1
        self.n_orbit_choices = len(self.ORBIT_CHOICES)
        self.action_dim = self.ACT_GOTO_INC_START + self.n_orbit_choices
        
        self.obs_dim = 14 
        self.global_state_dim = self.obs_dim * self.n_sats 
        
        # C·∫•u h√¨nh m·∫∑t tr·ªùi m·∫∑c ƒë·ªãnh
        self.SUN_DIRECTION = np.array([1.0, 0.0, 0.0])

        self._build_locations()
        self._init_simulator()

    def _build_locations(self):
            rng = np.random.RandomState(42)
            self.targets_ecef = []
            
            if self.hurricane_mode:
                print("[INFO] K√≠ch ho·∫°t k·ªãch b·∫£n: HURRICANE (T·∫≠p trung m·ª•c ti√™u)")
                # T·∫°o v√πng b√£o t·∫≠p trung (V√≠ d·ª•: V√πng bi·ªÉn Caribe/ƒê·∫°i T√¢y D∆∞∆°ng)
                # Lat: 15 ƒë·∫øn 35 ƒë·ªô B·∫Øc, Lon: -90 ƒë·∫øn -60 ƒë·ªô T√¢y
                center_lat, center_lon = 25.0, -75.0
                
                for _ in range(self.n_targets):
                    # Ph√¢n ph·ªëi Gaussian xung quanh t√¢m b√£o ƒë·ªÉ t·∫°o cluster
                    lat = np.clip(rng.normal(center_lat, 5.0), -90, 90)
                    lon = np.clip(rng.normal(center_lon, 10.0), -180, 180)
                    self.targets_ecef.append(self._lld_to_ecef(lat*macros.D2R, lon*macros.D2R, 0))
            else:
                print("[INFO] K√≠ch ho·∫°t k·ªãch b·∫£n: GLOBAL RANDOM")
                for _ in range(self.n_targets):
                    lat, lon = rng.uniform(-10, 85), rng.uniform(-180, 180)
                    self.targets_ecef.append(self._lld_to_ecef(lat*macros.D2R, lon*macros.D2R, 0))
                    
            self.gs_ecef = []
            for lat, lon in self.gs_coords:
                self.gs_ecef.append(self._lld_to_ecef(lat*macros.D2R, lon*macros.D2R, 0))

    def _lld_to_ecef(self, lat, lon, alt):
        r = self.earth_radius + alt
        x = r * math.cos(lat) * math.cos(lon)
        y = r * math.cos(lat) * math.sin(lon)
        z = r * math.sin(lat)
        return np.array([x, y, z])

    def _cleanup(self):
        if hasattr(self, 'scSim'): self.scSim = None
        gc.collect()

    def _init_simulator(self):
        self._cleanup()
        self.scSim = SimulationBaseClass.SimBaseClass()
        self.scSim.SetProgressBar(False)
        self.taskName = "dynTask"
        self.decision_dt = 10.0  # Th·ªùi gian gi·ªØa c√°c quy·∫øt ƒë·ªãnh h√†nh ƒë·ªông  
        self.dt = macros.sec2nano(2.0) 
        self.scSim.CreateNewProcess("dynProcess").addTask(self.scSim.CreateNewTask(self.taskName, self.dt))

        self.all_viz_objects = [] 
        self.sats = []
        self.states = []
        
        # [QUAN TR·ªåNG] Reset danh s√°ch n√†y ngay ƒë·∫ßu h√†m
        self.force_msgs = []
        self.force_payloads = [] 
        
        self.SUN_DIRECTION = np.array([1.0, 0.0, 0.0])
    
# --- C√ÄI ƒê·∫∂T SPICE (L·∫•y d·ªØ li·ªáu thi√™n vƒÉn th·∫≠t) ---
# --- [FIXED] SPICE SETUP (ƒê∆Ø·ªúNG D·∫™N C·ª®NG) ---
        self.spiceObject = spiceInterface.SpiceInterface()
        self.spiceObject.ModelTag = "SpiceInterface"
        
        self.spiceObject.zeroBase = "Earth"
        # ƒê∆∞·ªùng d·∫´n ch√≠nh x√°c b·∫°n ƒë√£ cung c·∫•p
        # L∆∞u √Ω: D√πng r"..." ƒë·ªÉ Python hi·ªÉu l√† raw string (tr√°nh l·ªói d·∫•u \)
        spiceDataPath = r"D:\ProjectIII\Code\Multi-Sat-MARL-2025\basilisk\supportData\EphemerisData"
        kernelName = "de430.bsp"
        
        # Ki·ªÉm tra l·∫ßn cu·ªëi cho ch·∫Øc ƒÉn
        full_path = os.path.join(spiceDataPath, kernelName)
        if not os.path.exists(full_path):
            print(f"[CRITICAL ERROR] V·∫´n kh√¥ng th·∫•y file t·∫°i: {full_path}")
            print("H√£y ki·ªÉm tra l·∫°i xem file de430.bsp c√≥ th·ª±c s·ª± n·∫±m ·ªü ƒë√≥ kh√¥ng!")
            sys.exit(1)
            
        print(f"[INFO] ƒêang load SPICE kernel t·ª´: {full_path}")
        self.spiceObject.SPICEDataPath = spiceDataPath
        self.spiceObject.addPlanetNames(["sun", "earth"])
        self.spiceObject.loadSpiceKernel(kernelName, spiceDataPath)
        
        # C√†i ƒë·∫∑t th·ªùi gian b·∫Øt ƒë·∫ßu (V√≠ d·ª•: NƒÉm 2025)
        self.spiceObject.UTCCalInit = "2025 December 18 00:00:00.0"
        
        self.scSim.AddModelToTask(self.taskName, self.spiceObject)
        
        gravFactory = simIncludeGravBody.gravBodyFactory()
        earth = gravFactory.createEarth(); earth.isCentralBody = True
        self.planet_mu = earth.mu
        
        earth.planetBodyInMsg.subscribeTo(self.spiceObject.planetStateOutMsgs[1])
        
        self.sats, self.states, self.force_msgs, self.force_payloads, self.all_viz_objects = [], [], [], [], []
        
        # --- A. T·∫†O V·ªÜ TINH ---
        for i in range(self.n_sats):
            sc = spacecraft.Spacecraft()
            sc.ModelTag = f"Sat_{i}"
            sc.hub.mHub = 500.0 
            self.scSim.AddModelToTask(self.taskName, sc)
            gravFactory.addBodiesTo(sc)
            
            # EXT FORCE SETUP
            extForce = extForceTorque.ExtForceTorque()
            extForce.ModelTag = f"ExtForce_{i}"
            sc.addDynamicEffector(extForce)
            self.scSim.AddModelToTask(self.taskName, extForce)
            
            # [FIX L·ªñI INDEX ERROR] Append ngay trong v√≤ng l·∫∑p
            cmdPayload = messaging.CmdForceInertialMsgPayload()
            cmdPayload.forceRequestInertial = [0.0, 0.0, 0.0]
            cmdMsg = messaging.CmdForceInertialMsg().write(cmdPayload)
            
            extForce.cmdForceInertialInMsg.subscribeTo(cmdMsg)
            
            self.force_msgs.append(cmdMsg)
            self.force_payloads.append(cmdPayload)
            
            # ORBIT SETUP
# [C·∫¨P NH·∫¨T] Orbit Parameter chu·∫©n b√†i b√°o (LEO ~600km)
            oe = orbitalMotion.ClassicElements()
            oe.a = self.earth_radius + self.SAT_ALTITUDE # 600km Altitude
            oe.e = 0.001
            oe.i = 45.0 * macros.D2R # Inclination qu√©t qua v√πng b√£o t·ªët
            oe.Omega = (i * 360.0 / self.n_sats) * macros.D2R
            oe.omega = 0.0; oe.f = 0.0
            
            rN, vN = orbitalMotion.elem2rv(self.planet_mu, oe)
            sc.hub.r_CN_NInit = np.array(rN); sc.hub.v_CN_NInit = np.array(vN)
            
            self.sats.append(sc); self.all_viz_objects.append(sc)
            bore_vec = -np.array(rN) / np.linalg.norm(rN)
            self.states.append({'fuel': self.fuel_max, 'batt': self.batt_max, 'buffer': 0, 'bore_vec': bore_vec})

        # --- B. DUMMY OBJECTS ---
# --- B. DUMMY OBJECTS ---
        for k, ecef in enumerate(self.targets_ecef):
            dummy = spacecraft.Spacecraft()
            dummy.ModelTag = f"TGT_{k}"
            r_mag = np.linalg.norm(ecef)
            dummy.hub.r_CN_NInit = ecef * ((r_mag + 50000.0) / r_mag)
            self.scSim.AddModelToTask(self.taskName, dummy)
            self.all_viz_objects.append(dummy)

        # [FIX L·ªñI INDEX ERROR]
        # C·∫≠p nh·∫≠t t√™n cho ƒë√∫ng v·ªõi 5 tr·∫°m trong __init__ (Svalbard, Matera, Hanoi, Sydney, NY)
        gs_names = ["Svalbard", "Matera", "Hanoi", "Sydney", "NY"]
        
        for k, ecef in enumerate(self.gs_ecef):
            dummy = spacecraft.Spacecraft()
            
            # C∆° ch·∫ø an to√†n: N·∫øu k v∆∞·ª£t qu√° danh s√°ch t√™n, d√πng s·ªë th·ª© t·ª± ƒë·ªÉ kh√¥ng b·ªã crash
            if k < len(gs_names):
                tag_name = gs_names[k]
            else:
                tag_name = f"{k}"
            
            dummy.ModelTag = f"GS_{tag_name}"
            
            r_mag = np.linalg.norm(ecef)
            dummy.hub.r_CN_NInit = ecef * ((r_mag + 400000.0) / r_mag)
            self.scSim.AddModelToTask(self.taskName, dummy)
            self.all_viz_objects.append(dummy)

        if self.viz:
            try:
                self.vizTaskName = "vizTask"
                self.scSim.CreateNewProcess("vizProcess").addTask(self.scSim.CreateNewTask(self.vizTaskName, macros.sec2nano(10.0)))
                viz = vizSupport.enableUnityVisualization(
                    self.scSim, 
                    self.vizTaskName,  # <--- S·ª¨A CH·ªñ N√ÄY
                    self.all_viz_objects, 
                    saveFile="mission_sim_optimize_reward_sun.bin"
                )
                                
                viz.settings.showSpacecraftLabels = 1
                viz.settings.showOrbitLines = 1                                
                # (T√πy ch·ªçn) Hi·ªÉn th·ªã ng√†y gi·ªù th·ª±c t·ª´ SPICE
                # viz.epochInMsg.subscribeTo(self.spiceObject.epochProtoOutMsg)         
                self.vizObj = viz
            except Exception as e: 
                print(f"Viz Error: {e}")
                self.vizObj = None
                
        self.scSim.InitializeSimulation()
        self.sim_time = 0.0

    def reset(self):
        self.global_captured_targets = set()
        self._init_simulator()
        return self._get_all_obs()

    def _get_all_obs(self):
        obs_list = []
        for i, sat in enumerate(self.sats):
            stateMsg = sat.scStateOutMsg.read()
            r_sat = np.array(stateMsg.r_BN_N)
            v_sat = np.array(stateMsg.v_BN_N)
            
            # --- SMART OBS: T√åM TR·∫†M G·∫¶N NH·∫§T ---
            min_dist = 1e9
            nearest_gs_vec = np.zeros(3)
            for gs_pos in self.gs_ecef:
                rel_vec = gs_pos - r_sat
                dist = np.linalg.norm(rel_vec)
                if dist < min_dist:
                    min_dist = dist
                    nearest_gs_vec = rel_vec
            
            dist_norm = [min_dist / 10000e3] 
            if min_dist > 0: dir_norm = nearest_gs_vec / min_dist
            else: dir_norm = [0, 0, 0]

            try:
                oe = orbitalMotion.rv2elem(self.planet_mu, r_sat, v_sat)
                inc_norm = [oe.i / (90.0 * macros.D2R)]
            except: inc_norm = [0.5]
            
            r_norm = r_sat / 1e7; v_norm = v_sat / 1e4
            f = [self.states[i]['fuel'] / self.fuel_max]
            b = [self.states[i]['batt'] / self.batt_max]
            d = [self.states[i]['buffer'] / self.buffer_max]
            
            obs = np.concatenate([r_norm, v_norm, f, b, d, inc_norm, dist_norm, dir_norm]).astype(np.float32)
            obs_list.append(obs)
        return obs_list

    def step(self, actions, debug_mode=True):
        rewards = np.zeros(self.n_sats)
        dones = [False] * self.n_sats
        current_nano = macros.sec2nano(self.sim_time)
        
        # --- [NEW] C·∫¨P NH·∫¨T VECTOR M·∫∂T TR·ªúI T·ª™ SPICE ---
        # SPICE tr·∫£ v·ªÅ t·ªça ƒë·ªô trong h·ªá Inertial
        # msg[0] l√† Sun, msg[1] l√† Earth (do th·ª© t·ª± addPlanetNames)
        sunMsg = self.spiceObject.planetStateOutMsgs[0].read()
        earthMsg = self.spiceObject.planetStateOutMsgs[1].read()
        
        r_Sun_N = np.array(sunMsg.PositionVector)
        r_Earth_N = np.array(earthMsg.PositionVector)
        
        # Vector t·ª´ Tr√°i ƒë·∫•t -> M·∫∑t tr·ªùi
        real_sun_vec = r_Sun_N - r_Earth_N
        # C·∫≠p nh·∫≠t bi·∫øn class ƒë·ªÉ d√πng cho logic b√™n d∆∞·ªõi
        self.SUN_DIRECTION = real_sun_vec / (np.linalg.norm(real_sun_vec) + 1e-9)
        
        for i, action in enumerate(actions):
            sat = self.sats[i]
            state = self.states[i]
            act = int(action)
            
            # 1. Reset L·ª±c = 0
            self.force_payloads[i].forceRequestInertial = [0.0, 0.0, 0.0]
            self.force_msgs[i].write(self.force_payloads[i], current_nano)

            # --- CHECK CH·∫æT & AN TO√ÄN ---
            scMsg = sat.scStateOutMsg.read()
            r_curr = np.array(scMsg.r_BN_N)
            v_curr = np.array(scMsg.v_BN_N)
            
            # Ph·∫°t n·∫∑ng n·∫øu r∆°i ho·∫∑c NaN
            if np.isnan(r_curr).any() or (np.linalg.norm(r_curr) - self.earth_radius) < 200e3:
                dones[i] = True; rewards[i] -= 50.0
                if debug_mode: print(f"[t={self.sim_time:.0f}] üí• Sat {i} DESTROYED (r={np.linalg.norm(r_curr)/1e3:.1f} km)")
                continue
                
            
            if state['batt'] <= 0 or state['fuel'] <= 0:
                dones[i] = True; rewards[i] -= 20.0
                if debug_mode: print(f"[t={self.sim_time:.0f}] ‚ö†Ô∏è Sat {i} OUT OF RESOURCES (Fuel: {state['fuel']:.1f}, Batt: {state['batt']:.1f})")
                continue

            v_mag = np.linalg.norm(v_curr)
            v_dir = v_curr / (v_mag + 1e-9)

            # --- LOGIC H√ÄNH ƒê·ªòNG ---
            
            # 1. CHARGE
# 1. CHARGE
            if act == self.ACT_CHARGE:
                # Ki·ªÉm tra xem V·ªÜ TINH c√≥ ƒëang ·ªü ngo√†i s√°ng kh√¥ng
                sat_pos_dir = r_curr / np.linalg.norm(r_curr)
                sun_dir = self.SUN_DIRECTION
                sat_in_sun = np.dot(sat_pos_dir, sun_dir) > -0.1
                
                if sat_in_sun:
                    state['batt'] = min(self.batt_max, state['batt'] + 5.0)
                    if state['batt'] < 30.0: rewards[i] += 1.0
                    if debug_mode: print(f"[t={self.sim_time:.0f}] ‚ö° Sat {i} CHARGE (+1) from {state['batt']-5:.1f} to {state['batt']:.1f}")
                else:
                    # S·∫°c trong b√≥ng t·ªëi -> V√¥ d·ª•ng -> Ph·∫°t
                    rewards[i] -= 0.2
                    if debug_mode and i==0: print(f"[t={self.sim_time:.0f}] üåë Sat {i} Cannot Charge in Eclipse!")

            # 2. ORBIT CHANGE
            elif act in [self.ACT_ALT_UP, self.ACT_ALT_DOWN] or \
                 (self.ACT_GOTO_INC_START <= act < self.ACT_GOTO_INC_START + self.n_orbit_choices):
                
                # [STRATEGY] N·∫øu Buffer ƒë·∫ßy, th∆∞·ªüng cho vi·ªác di chuy·ªÉn (khuy·∫øn kh√≠ch v·ªÅ tr·∫°m)
                if state['buffer'] >= self.buffer_max: 
                    rewards[i] += 2.0 
                    if debug_mode: print(f"[t={self.sim_time:.0f}] üöÄ Sat {i} Move (full buffer)")
                else: 
                    rewards[i] -= 0.1 # Ph·∫°t nh·∫π n·∫øu di chuy·ªÉn lung tung khi r·∫£nh
                    if debug_mode: print(f"[t={self.sim_time:.0f}] üîÑ Sat {i} Move (no buffer)")

                # X·ª≠ l√Ω L·ª±c
                if act in [self.ACT_ALT_UP, self.ACT_ALT_DOWN]:
                     if state['fuel'] >= 1.0:
                        dv_vec = v_dir if act == self.ACT_ALT_UP else -v_dir
                        force_vec = (sat.hub.mHub * 10.0) / self.decision_dt * dv_vec
                        self.force_payloads[i].forceRequestInertial = force_vec.tolist()
                        self.force_msgs[i].write(self.force_payloads[i], current_nano)
                        state['fuel'] -= 0.1 # Cost th·∫•p ƒë·ªÉ AI d√°m th·ª≠
                        if debug_mode: print(f"[t={self.sim_time:.0f}] üîºüîΩ Sat {i} Burn ALT")
                
                elif self.ACT_GOTO_INC_START <= act:
                    if state['fuel'] >= 5.0:
                        choice_idx = act - self.ACT_GOTO_INC_START
                        target_inc_rad = self.ORBIT_CHOICES[choice_idx] * macros.D2R
                        
                        oe = orbitalMotion.rv2elem(self.planet_mu, r_curr, v_curr)
                        inc_diff = target_inc_rad - oe.i
                        
                        if abs(inc_diff) < 2.0 * macros.D2R: rewards[i] += 0.5 # ƒê√£ ·ªü ƒë√∫ng qu·ªπ ƒë·∫°o
                        else:
                            h_vec = np.cross(r_curr, v_curr)
                            h_dir = h_vec / np.linalg.norm(h_vec)
                            burn_dir = h_dir if inc_diff > 0 else -h_dir
                            
                            req_dv = 2 * v_mag * math.sin(abs(inc_diff) / 2)
                            apply_dv = min(req_dv, 50.0)
                            
                            force_vec = (sat.hub.mHub * apply_dv) / self.decision_dt * burn_dir
                            self.force_payloads[i].forceRequestInertial = force_vec.tolist()
                            self.force_msgs[i].write(self.force_payloads[i], current_nano)
                            state['fuel'] -= 2.0 # Cost trung b√¨nh
                            if debug_mode: print(f"[t={self.sim_time:.0f}] üìê Sat {i} Burn INC")

            # 3. CAPTURE (CH·ª§P ·∫¢NH)
            elif 0 <= act < self.n_targets:
                # L∆∞u √Ω: Kh√¥ng tr·ª´ pin c·ªë ƒë·ªãnh ·ªü ƒë√¢y n·ªØa, m√† tr·ª´ theo h√†nh ƒë·ªông c·ª• th·ªÉ b√™n d∆∞·ªõi
                
                # A. Check Buffer (B·ªô nh·ªõ)
                if state['buffer'] >= self.buffer_max:
                    rewards[i] -= 2.0
                    if debug_mode: print(f"[t={self.sim_time:.0f}] ‚ö†Ô∏è Sat {i} Buffer FULL, cannot capture (-2.0)")
                
                # B. Check ƒë√£ ch·ª•p ch∆∞a
                elif act in self.global_captured_targets: 
                    rewards[i] -= 0.5 # Ph·∫°t nh·∫π v√¨ l√£ng ph√≠ th·ªùi gian ch·ª•p l·∫°i
                
                else:
                    tgt_pos = self.targets_ecef[act]
                    req_vec = tgt_pos - r_curr
                    dist = np.linalg.norm(req_vec); req_vec /= (dist + 1e-9)
                    
                    # --- CHECK √ÅNH S√ÅNG (DAY/NIGHT) ---
                    # D√πng vector M·∫∑t tr·ªùi th·∫≠t (ƒë√£ c·∫≠p nh·∫≠t t·ª´ SPICE ·ªü ƒë·∫ßu h√†m step)
                    sun_dir = self.SUN_DIRECTION
                    target_normal = tgt_pos / np.linalg.norm(tgt_pos)
                    
                    # Ng∆∞·ª°ng -0.1 cho ph√©p ch·ª•p l√∫c b√¨nh minh/ho√†ng h√¥n
                    is_daylight = np.dot(target_normal, sun_dir) > -0.1
                    
                    if not is_daylight:
                        rewards[i] -= 0.1 
                        if debug_mode: print(f"Sat {i} Target {act} is in DARKNESS")
                    else:
                        # --- T√çNH TO√ÅN G√ìC XOAY ---
                        cos = np.dot(state['bore_vec'], req_vec)
                        angle = math.degrees(math.acos(np.clip(cos, -1.0, 1.0)))
                        max_turn = math.degrees(self.max_slew_rate) * self.decision_dt
                        
                        # TR∆Ø·ªúNG H·ª¢P 1: ƒê√É H∆Ø·ªöNG V√ÄO M·ª§C TI√äU (C√≥ th·ªÉ ch·ª•p)
                        if angle <= max_turn:
                            state['bore_vec'] = req_vec # Kh√≥a m·ª•c ti√™u
                            state['batt'] -= 1.0        # [NEW] Ch·ª•p ·∫£nh t·ªën nhi·ªÅu pin (Sensor b·∫≠t)
                            
                            # T√≠nh g√≥c Off-Nadir hi·ªán t·∫°i
                            nadir_vec = -r_curr / np.linalg.norm(r_curr)
                            off_nadir = math.degrees(math.acos(np.clip(np.dot(req_vec, nadir_vec), -1, 1)))
                            
                            # --- [QUAN TR·ªåNG] ƒêI·ªÄU KI·ªÜN M·ªöI: 1500km & 45 ƒë·ªô ---
                            # S·ª≠ d·ª•ng bi·∫øn self.CAPTURE_MAX_DIST (ƒë√£ set 1500e3) v√† self.CAPTURE_MAX_OFF_NADIR (ƒë√£ set 45.0)
                            # Ho·∫∑c hardcode tr·ª±c ti·∫øp nh∆∞ d∆∞·ªõi ƒë√¢y ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√∫ng y√™u c·∫ßu:
                            valid_dist = dist < 1500e3  # <--- M·ªöI: 1500 km
                            valid_angle = off_nadir < 45.0 # <--- M·ªöI: 45 ƒë·ªô
                            
                            if valid_dist and valid_angle:
                                self.global_captured_targets.add(act)
                                state['buffer'] += 1
                                rewards[i] += 10.0 # Th∆∞·ªüng l·ªõn (+10)
                                if debug_mode: print(f"[t={self.sim_time:.0f}] üì∏ Sat {i} CAPTURED T{act} (Dist: {dist/1e3:.0f}km, Off-Nadir: {off_nadir:.1f}¬∞)")
                            else: 
                                rewards[i] -= 0.1
                                if debug_mode: print(f"[t={self.sim_time:.0f}] ‚ö†Ô∏è Sat {i} T{act} FAILED (Dist: {dist/1e3:.0f}km, Off-Nadir: {off_nadir:.0f}¬∞)")
                        
                        # TR∆Ø·ªúNG H·ª¢P 2: ƒêANG XOAY (Slewing)
                        else:
                            # Xoay camera t·ªën √≠t pin h∆°n ch·ª•p (Reaction Wheels)
                            state['batt'] -= 0.2 # [NEW] Tr·ª´ hao pin khi xoay
                            
                            # C·∫≠p nh·∫≠t vector h∆∞·ªõng (Slew)
                            ratio = max_turn / angle
                            state['bore_vec'] = (1-ratio)*state['bore_vec'] + ratio*req_vec
                            state['bore_vec'] /= np.linalg.norm(state['bore_vec'])
                            
                            rewards[i] += 0.05 # Th∆∞·ªüng nh·ªè khuy·∫øn kh√≠ch xoay
                            if debug_mode: print(f"[t={self.sim_time:.0f}] üîÑ Sat {i} Slewing to T{act} ({angle:.0f}¬∞ left)")

            # 4. DOWNLINK (TRUY·ªÄN TIN)
            elif self.n_targets <= act < self.n_targets + self.n_gs:
                # L∆∞u √Ω: Kh√¥ng tr·ª´ pin/xƒÉng c·ªë ƒë·ªãnh ·ªü ƒë√¢y (theo logic m·ªõi)
                
                gs_idx = act - self.n_targets
                gs_pos = self.gs_ecef[gs_idx]
                req_vec = gs_pos - r_curr
                dist = np.linalg.norm(req_vec); req_vec /= (dist + 1e-9)
                
                cos = np.dot(state['bore_vec'], req_vec)
                angle = math.degrees(math.acos(np.clip(cos, -1.0, 1.0)))
                max_turn = math.degrees(self.max_slew_rate) * self.decision_dt
                
                is_full = (state['buffer'] >= self.buffer_max)
                if is_full: 
                    rewards[i] += 1.0 
                    if debug_mode: print(f"[t={self.sim_time:.0f}] üöÄ Sat {i} Move (full buffer)")
                
                # TR∆Ø·ªúNG H·ª¢P 1: ƒê√É H∆Ø·ªöNG V·ªÄ TR·∫†M (G√≥c l·ªách nh·ªè)
                if angle <= max_turn:
                    state['bore_vec'] = req_vec
                    if state['buffer'] > 0:
                        # [FIX] SUCCESS CHECK: D√πng gi·ªõi h·∫°n v·∫≠t l√Ω 2200km
                        if dist < 2200e3: 
                            state['buffer'] -= 1
                            state['batt'] -= 1.0 # Downlink t·ªën nhi·ªÅu pin (Transmitter ON)
                            rewards[i] += 49.0   # JACKPOT
                            if debug_mode: print(f"[t={self.sim_time:.0f}] üì° Sat {i} DOWNLINK SUCCESS (+49) Dist: {dist/1e3:.0f}km")
                        
                        # [FIX] APPROACHING: D√πng gi·ªõi h·∫°n 3000km (V·ª´a qua ƒë∆∞·ªùng ch√¢n tr·ªùi)
                        elif dist < 3000e3:
                            # Shaping reward: C√†ng g·∫ßn 2200km c√†ng nhi·ªÅu ƒëi·ªÉm
                            score = (3000e3 - dist) / (3000e3 - 2200e3)
                            rewards[i] += score * (2.0 if is_full else 0.5)
                            if debug_mode: print(f"[t={self.sim_time:.0f}] üì° Sat {i} APPROACHING (+{score:.2f}) Dist: {dist/1e3:.0f}km")
                        else: 
                            rewards[i] -= 0.1
                            if debug_mode: print(f"[t={self.sim_time:.0f}] üì° Sat {i} TOO FAR ({dist/1e3:.0f} km)")
                    else: 
                        rewards[i] -= 0.5 # Buffer r·ªóng
                        if debug_mode: print(f"[t={self.sim_time:.0f}] ‚ö†Ô∏è Sat {i} Buffer EMPTY")
                
                # TR∆Ø·ªúNG H·ª¢P 2: ƒêANG XOAY (Slewing)
                else:
                    state['batt'] -= 0.2 # Tr·ª´ pin cho Reaction Wheels (th·ªëng nh·∫•t v·ªõi Capture)
                    
                    ratio = max_turn / angle
                    state['bore_vec'] = (1-ratio)*state['bore_vec'] + ratio*req_vec
                    state['bore_vec'] /= np.linalg.norm(state['bore_vec'])
                    
                    rewards[i] += 0.05 
                    if debug_mode: print(f"[t={self.sim_time:.0f}] üîÑ Sat {i} Slewing to GS{gs_idx} ({angle:.0f}¬∞)")
            
            # Drain nh·∫π
            else:
                state['batt'] -= 0.05

        stop_time = self.sim_time + self.decision_dt
        self.scSim.ConfigureStopTime(int(macros.sec2nano(stop_time)))
        self.scSim.ExecuteSimulation()
        self.sim_time = stop_time
        
        if self.sim_time > 86400: dones = [True] * self.n_sats
        return self._get_all_obs(), rewards, dones, {}

# --- 4. TRAINER ---
# --- C√ÅC H√ÄM PH·ª§ TR·ª¢ L∆ØU/T·∫¢I MODEL ---
def save_checkpoint(path, agent, optimizer, epoch, history):
    torch.save({
        'epoch': epoch,
        'model_state_dict': agent.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'history': history
    }, path)
    print(f"[CHECKPOINT] ƒê√£ l∆∞u model t·∫°i epoch {epoch} v√†o '{path}'")

def load_checkpoint(path, agent, optimizer):
    if os.path.exists(path):
        # [S·ª¨A L·ªñI T·∫†I ƒê√ÇY] Th√™m weights_only=False ƒë·ªÉ cho ph√©p load d·ªØ li·ªáu numpy/list
        try:
            checkpoint = torch.load(path, weights_only=False)
        except TypeError:
            # Fallback cho c√°c phi√™n b·∫£n PyTorch c≈© h∆°n (ch∆∞a c√≥ tham s·ªë weights_only)
            checkpoint = torch.load(path)
            
        agent.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        history = checkpoint.get('history', {
            'epoch': [], 'total_reward': [], 'targets_captured': [], 
            'data_downlinked': [], 'avg_fuel_remaining': []
        })
        print(f"[CHECKPOINT] ƒê√£ t√¨m th·∫•y file l∆∞u! Ti·∫øp t·ª•c train t·ª´ Epoch {start_epoch}")
        return start_epoch, history
    else:
        print("[CHECKPOINT] Kh√¥ng t√¨m th·∫•y file l∆∞u. B·∫Øt ƒë·∫ßu train m·ªõi.")
        return 0, {
            'epoch': [], 'total_reward': [], 'targets_captured': [], 
            'data_downlinked': [], 'avg_fuel_remaining': []
        }

# --- 4. TRAINER (C√ì T√çNH NƒÇNG SAVE/LOAD) ---
def train_mission():
    N_SATS = 4
    N_TARGETS = 100 
    MAX_EPOCHS = 1000
    CHECKPOINT_FILE = "mappo_hurricane_checkpoint.pth" # T√™n file l∆∞u
    
    print("--- 1. TRAINING (HURRICANE SCENARIO + CHECKPOINT) ---")
    # K√≠ch ho·∫°t Hurricane Mode
    env = BasiliskFullMissionEnv(n_sats=N_SATS, n_targets=N_TARGETS, viz=False, hurricane_mode=True)
    agent = MultiAgentActorCritic(env.obs_dim, env.global_state_dim, env.action_dim)
    optimizer = optim.Adam(agent.parameters(), lr=3e-4)
    
    # --- [NEW] LOAD CHECKPOINT N·∫æU C√ì ---
    start_epoch, history = load_checkpoint(CHECKPOINT_FILE, agent, optimizer)
    
    for ep in range(start_epoch, MAX_EPOCHS): 
        obs_list = env.reset()
        ep_rewards = np.zeros(N_SATS)
        done = False
        
        # Metrics
        targets_captured_count = 0
        data_downlinked_count = 0
        
        batch_obs, batch_gs, batch_acts, batch_rews = [], [], [], []
        
        while not done:
            actions_t = []
            global_state_t = np.concatenate(obs_list)
            
            for i in range(N_SATS):
                obs_t = torch.FloatTensor(obs_list[i]).unsqueeze(0)
                logits = agent.act(obs_t)
                action = torch.distributions.Categorical(logits=logits).sample().item()
                actions_t.append(action)
            
            # Step
            next_obs, rewards, dones, info = env.step(actions_t, debug_mode=False)
            
            # Metrics ƒë·∫øm t·∫°m qua reward (ƒë·ªÉ log)
            for r in rewards:
                if r >= 9.0: targets_captured_count += 1 
                if r >= 49.0: data_downlinked_count += 1
            
            batch_obs.append(obs_list)
            batch_gs.append(global_state_t)
            batch_acts.append(actions_t)
            batch_rews.append(rewards)
            
            obs_list = next_obs
            ep_rewards += rewards
            done = any(dones)
        
        # --- PPO UPDATE ---
        optimizer.zero_grad()
        returns = np.zeros_like(batch_rews)
        running_add = np.zeros(N_SATS)
        for t in reversed(range(len(batch_rews))):
            running_add = batch_rews[t] + 0.99 * running_add
            returns[t] = running_add
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        ent_loss, act_loss, val_loss = 0, 0, 0
        for t in range(len(batch_obs)):
            g_state = torch.FloatTensor(batch_gs[t]).unsqueeze(0)
            avg_return = np.mean(returns[t])
            target_val = torch.FloatTensor([avg_return]).unsqueeze(0)
            val_pred = agent.evaluate(g_state)
            val_loss += nn.MSELoss()(val_pred, target_val)
            
            for i in range(N_SATS):
                obs = torch.FloatTensor(batch_obs[t][i]).unsqueeze(0)
                act = torch.tensor([batch_acts[t][i]])
                logits = agent.act(obs)
                dist = torch.distributions.Categorical(logits=logits)
                ent_loss += dist.entropy()
                act_loss += -dist.log_prob(act) * (returns[t][i] - val_pred.item())
        
        total_loss = act_loss + 0.5 * val_loss - 0.01 * ent_loss
        total_loss.backward()
        nn.utils.clip_grad_norm_(agent.parameters(), 0.5)
        optimizer.step()
        
        # --- LOGGING ---
        avg_fuel = np.mean([s['fuel'] for s in env.states])
        total_rw = np.sum(ep_rewards)
        real_capture_count = len(env.global_captured_targets)
        
        # 1. C·∫≠p nh·∫≠t v√†o bi·∫øn History (ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì v√† l∆∞u checkpoint)
        history['epoch'].append(ep+1)
        history['total_reward'].append(total_rw)
        history['targets_captured'].append(real_capture_count)
        history['data_downlinked'].append(data_downlinked_count)
        history['avg_fuel_remaining'].append(avg_fuel)
        
        print(f"Epoch {ep+1} | Reward: {total_rw:.1f} | Captured: {real_capture_count} | Downlink: {data_downlinked_count} | Fuel: {avg_fuel:.1f}%")
        
        # 2. L∆ØU CHECKPOINT (Quan tr·ªçng ƒë·ªÉ l·∫ßn sau load l·∫°i)
        save_checkpoint(CHECKPOINT_FILE, agent, optimizer, ep, history)

        # 3. [M·ªöI] L∆ØU CSV THEO CH·∫æ ƒê·ªò APPEND (GHI N·ªêI TI·∫æP)
        # T·∫°o m·ªôt DataFrame ch·ªâ ch·ª©a th√¥ng tin c·ªßa Epoch hi·ªán t·∫°i
        current_epoch_data = {
            'epoch': [ep+1],
            'total_reward': [total_rw],
            'targets_captured': [real_capture_count],
            'data_downlinked': [data_downlinked_count],
            'avg_fuel_remaining': [avg_fuel]
        }
        df_epoch = pd.DataFrame(current_epoch_data)
        
        # Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
        csv_file = "comparison_results.csv"
        file_exists = os.path.isfile(csv_file)
        
        try:
            # mode='a' l√† append (ghi n·ªëi ƒëu√¥i)
            # header=not file_exists: Ch·ªâ ghi t√™n c·ªôt n·∫øu file ch∆∞a t·ªìn t·∫°i
            df_epoch.to_csv(csv_file, mode='a', header=not file_exists, index=False)
        except PermissionError:
            print(f"[WARNING] Kh√¥ng th·ªÉ l∆∞u CSV epoch {ep+1} v√¨ file ƒëang m·ªü! ƒê·ª´ng lo, Checkpoint v·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u.")

    # --- CU·ªêI H√ÄM TRAIN ---
    # Kh√¥ng c·∫ßn l∆∞u CSV t·ªïng ·ªü ƒë√¢y n·ªØa v√¨ ƒë√£ l∆∞u t·ª´ng b∆∞·ªõc r·ªìi
    print("\n[INFO] Ho√†n th√†nh hu·∫•n luy·ªán.")
    
    # V·∫Ω bi·ªÉu ƒë·ªì t·ª´ l·ªãch s·ª≠ ƒë·∫ßy ƒë·ªß
    # (L∆∞u √Ω: history v·∫´n ch·ª©a ƒë·ªß d·ªØ li·ªáu t·ª´ l√∫c load checkpoint)
    df_full = pd.DataFrame(history)
    plot_comparison(df_full)

    print("\n--- 2. DEMO (Viz Mode) ---")
    env = None; gc.collect()
    demo_env = BasiliskFullMissionEnv(n_sats=N_SATS, n_targets=N_TARGETS, viz=True)
    obs = demo_env.reset(); done = False
    
    try:
        while not done:
            acts = []
            for i in range(N_SATS):
                with torch.no_grad():
                    l = agent.act(torch.FloatTensor(obs[i]).unsqueeze(0))
                    acts.append(torch.argmax(l, dim=1).item())
            obs, _, d, _ = demo_env.step(acts, debug_mode=True)
            done = any(d)
    except KeyboardInterrupt: pass
    print("Done. Check 'mission_sim_optimize_reward.bin'")

import matplotlib.pyplot as plt

def plot_comparison(df):
    """
    V·∫Ω bi·ªÉu ƒë·ªì so s√°nh k·∫øt qu·∫£ hu·∫•n luy·ªán v·ªõi Benchmark c·ªßa b√†i b√°o.
    """
    plt.figure(figsize=(15, 5))
    
    # --- 1. Total Reward Comparison ---
    plt.subplot(1, 3, 1)
    plt.plot(df['epoch'], df['total_reward'], label='Our MAPPO Model', color='blue', linewidth=2)
    plt.title('Total Reward over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Reward')
    plt.grid(True, alpha=0.3)
    plt.legend()

    # --- 2. Targets Captured Comparison ---
    # Gi·∫£ s·ª≠ m√¥ h√¨nh b√†i b√°o (Heuristic/MIP) ƒë·∫°t ƒë∆∞·ª£c kho·∫£ng 75% s·ªë m·ª•c ti√™u trong k·ªãch b·∫£n B√£o
    PAPER_BASELINE_CAPTURE = 75.0 
    
    plt.subplot(1, 3, 2)
    plt.plot(df['epoch'], df['targets_captured'], label='Our MAPPO Model', marker='o', color='green')
    plt.axhline(y=PAPER_BASELINE_CAPTURE, color='red', linestyle='--', label='Paper Model (Benchmark)', linewidth=2)
    plt.title('Targets Captured (Hurricane Scenario)')
    plt.xlabel('Epoch')
    plt.ylabel('Number of Targets')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # --- 3. Data Downlinked vs Fuel ---
    plt.subplot(1, 3, 3)
    plt.plot(df['epoch'], df['data_downlinked'], label='Downlinked Packets', color='purple')
    
    # T·∫°o tr·ª•c Y th·ª© 2 cho nhi√™n li·ªáu
    ax2 = plt.gca().twinx()
    ax2.plot(df['epoch'], df['avg_fuel_remaining'], label='Fuel Remaining', color='orange', linestyle=':')
    ax2.set_ylabel('Fuel (%)')
    
    plt.title('Downlink Efficiency & Fuel')
    plt.xlabel('Epoch')
    plt.ylabel('Packets Downlinked')
    
    # G·ªôp legend
    lines, labels = plt.gca().get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    plt.legend(lines + lines2, labels + labels2, loc='upper left')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("hurricane_comparison_chart.png")
    print("[INFO] ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o 'hurricane_comparison_chart.png'")
    plt.show()

if __name__ == "__main__":
    train_mission()