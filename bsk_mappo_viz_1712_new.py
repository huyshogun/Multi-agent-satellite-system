import math
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import sys
import os

# --- 1. BASILISK IMPORTS ---
try:
    from Basilisk.utilities import SimulationBaseClass, macros, simIncludeGravBody, orbitalMotion, vizSupport, simIncludeRW
    from Basilisk.simulation import spacecraft, reactionWheelStateEffector
    BASILISK_AVAILABLE = True
    print("[INIT] Basilisk libraries loaded.")
except ImportError:
    print("[ERROR] Basilisk not found.")
    sys.exit(1)

# --- 2. MAPPO NETWORK (CTDE) ---
class MultiAgentActorCritic(nn.Module):
    def __init__(self, obs_dim, global_state_dim, action_dim, hidden=256):
        super().__init__()
        
        # ACTOR (Decentralized): Ch·ªâ nh·∫≠n Observation c·ª•c b·ªô c·ªßa ri√™ng Agent
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, action_dim)
        )
        
        # CRITIC (Centralized): Nh·∫≠n Global State (Th√¥ng tin c·ªßa to√†n b·ªô ƒë·ªôi h√¨nh)
        self.critic = nn.Sequential(
            nn.Linear(global_state_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, 1)
        )

    def act(self, obs):
        # H√†m n√†y d√πng khi ch·∫°y th·ª±c t·∫ø (Execution)
        return self.actor(obs)

    def evaluate(self, global_state):
        # H√†m n√†y ch·ªâ d√πng khi hu·∫•n luy·ªán (Training)
        return self.critic(global_state)

# --- 3. MULTI-AGENT ENVIRONMENT ---
class BasiliskMultiSatEnv:
    def __init__(self, n_sats=4, n_targets=50, decision_dt=10.0, viz=False):
        self.n_sats = n_sats
        self.n_targets = n_targets
        self.decision_dt = decision_dt
        self.viz = viz
        
        # H·∫±ng s·ªë v·∫≠t l√Ω
        self.earth_radius = 6371e3
        self.mu = 398600.4418e9
        self.max_slew_rate = math.radians(3.0)
        self.fuel_max = 100.0
        
        # Action Space: [Target 0...N-1, Idle, Thrust_Up, Thrust_Down]
        self.action_dim = self.n_targets + 3
        
        # Observation Dim (Local): [Pos(3), Vel(3), Fuel(1)] = 7 (R√∫t g·ªçn cho demo)
        self.obs_dim = 7 
        # Global State Dim: [Obs * n_sats] (Gh√©p t·∫•t c·∫£ obs c·ªßa c√°c v·ªá tinh l·∫°i)
        self.global_state_dim = self.obs_dim * self.n_sats 

        self._build_targets()
        self._init_simulator()

    def _build_targets(self):
        rng = np.random.RandomState(42)
        self.targets_ecef = []
        self.targets_lld = []
        for _ in range(self.n_targets):
            lat = rng.uniform(-60, 60) * macros.D2R
            lon = rng.uniform(-180, 180) * macros.D2R
            x = self.earth_radius * math.cos(lat) * math.cos(lon)
            y = self.earth_radius * math.cos(lat) * math.sin(lon)
            z = self.earth_radius * math.sin(lat)
            self.targets_ecef.append(np.array([x, y, z]))
            self.targets_lld.append([lat, lon, 0.0])

    def _cleanup(self):
        if hasattr(self, 'scSim'): self.scSim = None
        gc.collect()

    def _init_simulator(self):
        self._cleanup()
        self.scSim = SimulationBaseClass.SimBaseClass()
        self.scSim.SetProgressBar(False)
        self.processName = "dynProcess"
        self.taskName = "dynTask"
        self.dt = macros.sec2nano(1.0)
        
        dynProc = self.scSim.CreateNewProcess(self.processName)
        dynProc.addTask(self.scSim.CreateNewTask(self.taskName, self.dt))

        # --- T·∫°o 4 V·ªá tinh (Constellation) ---
        self.sats = []     # Danh s√°ch object v·ªá tinh Basilisk
        self.rw_effs = []  # Danh s√°ch b·ªô b√°nh ƒë√†
        self.states = []   # L∆∞u tr·∫°ng th√°i n·ªôi t·∫°i (Fuel, Attitude) cho t·ª´ng v·ªá tinh
        
        gravFactory = simIncludeGravBody.gravBodyFactory()
        earth = gravFactory.createEarth()
        earth.isCentralBody = True
        self.planet_mu = earth.mu
        
        rwFactory = simIncludeRW.rwFactory()

        # T·∫°o ƒë·ªôi h√¨nh Walker Delta (ph√¢n b·ªë ƒë·ªÅu c√°c m·∫∑t ph·∫≥ng qu·ªπ ƒë·∫°o)
        for i in range(self.n_sats):
            sc = spacecraft.Spacecraft()
            sc.ModelTag = f"Sat_{i}"
            self.scSim.AddModelToTask(self.taskName, sc)
            gravFactory.addBodiesTo(sc)
            
            # G·∫Øn RW
            rwStateEffector = reactionWheelStateEffector.ReactionWheelStateEffector()
            rwFactory.addToSpacecraft(f"RW_Array_{i}", rwStateEffector, sc)
            self.scSim.AddModelToTask(self.taskName, rwStateEffector)
            self.rw_effs.append(rwStateEffector)
            
            # C√†i ƒë·∫∑t qu·ªπ ƒë·∫°o (L·ªách nhau g√≥c RAAN ƒë·ªÉ ph·ªß to√†n c·∫ßu)
            oe = orbitalMotion.ClassicElements()
            oe.a = self.earth_radius + 400e3
            oe.e = 0.001
            oe.i = 45.0 * macros.D2R
            oe.Omega = (i * 360.0 / self.n_sats) * macros.D2R # RAAN l·ªách ƒë·ªÅu
            oe.omega = 0.0
            oe.f = 0.0
            rN, vN = orbitalMotion.elem2rv(self.planet_mu, oe)
            
            sc.hub.r_CN_NInit = np.array(rN)
            sc.hub.v_CN_NInit = np.array(vN)
            
            self.sats.append(sc)
            
            # Kh·ªüi t·∫°o tr·∫°ng th√°i n·ªôi t·∫°i (Fuel, H∆∞·ªõng nh√¨n)
            bore_vec = -np.array(rN) / np.linalg.norm(rN)
            self.states.append({
                'fuel': self.fuel_max,
                'bore_vec': bore_vec,
                'captured_targets': set() # M·ªói v·ªá tinh nh·ªõ n√≥ ƒë√£ ch·ª•p g√¨
            })

        # Viz Setup (ƒê√É S·ª¨A L·ªñI)
        if self.viz:
            try:
                viz = vizSupport.enableUnityVisualization(
                    self.scSim, self.taskName, self.sats, # Truy·ªÅn list v·ªá tinh v√†o
                    saveFile="mappo_constellation_new.bin"
                )
                
                # [FIXED] B·ªçc h√†m t·∫°o Cone trong try-except ƒë·ªÉ tr√°nh l·ªói phi√™n b·∫£n
                try:
                    if hasattr(vizSupport, 'createCone'):
                        for i in range(self.n_sats):
                            vizSupport.createCone(viz, fromBodyName=f"Sat_{i}", fov=30*macros.D2R, color=[0, 1, 0, 0.5])
                except Exception:
                    pass # B·ªè qua n·∫øu kh√¥ng h·ªó tr·ª£ v·∫Ω n√≥n
                
                # [FIXED] B·ªçc h√†m t·∫°o GroundStation
                try:
                    for k, lld in enumerate(self.targets_lld):
                        if hasattr(vizSupport, 'createGroundStation'):
                            vizSupport.createGroundStation(viz, groundPos=lld, stationName=f"TGT_{k}", color=[1, 0, 0, 1])
                except Exception: 
                    pass
                
                self.vizObj = viz
            except Exception as e:
                print(f"Viz Error (Ignored): {e}")
                self.vizObj = None

        self.scSim.InitializeSimulation()
        self.sim_time = 0.0

    def reset(self):
        self._init_simulator()
        return self._get_all_obs() # Tr·∫£ v·ªÅ list c√°c obs

    def _get_all_obs(self):
        obs_list = []
        for i, sat in enumerate(self.sats):
            r = np.array(sat.hub.r_CN_NInit).flatten() / 1e7
            v = np.array(sat.hub.v_CN_NInit).flatten() / 1e4
            f = [self.states[i]['fuel'] / self.fuel_max]
            # Obs c·ª•c b·ªô: [Pos(3), Vel(3), Fuel(1)] = 7
            obs = np.concatenate([r, v, f]).astype(np.float32)
            obs_list.append(obs)
        return obs_list
    '''
    def step(self, actions):
        rewards = np.zeros(self.n_sats)
        
        for i, action in enumerate(actions):
            sat = self.sats[i]
            state = self.states[i]
            act = int(action)
            
            # L·∫•y ƒë·ªô cao hi·ªán t·∫°i (Altitude)
            r_curr_vec = np.array(sat.hub.r_CN_NInit).flatten()
            r_mag = np.linalg.norm(r_curr_vec)
            altitude = r_mag - self.earth_radius # ƒê·ªô cao t√≠nh b·∫±ng m√©t

            v_curr = np.array(sat.hub.v_CN_NInit).flatten()
            v_dir = v_curr / (np.linalg.norm(v_curr) + 1e-9)

            # --- CHI·∫æN L∆Ø·ª¢C 1: GI·∫¢M PH·∫†T ---
            # Gi·∫£m penalty t·ª´ -0.5 xu·ªëng -0.05 ƒë·ªÉ AI "d√°m" d√πng xƒÉng h∆°n
            thrust_penalty = -0.05 
            
            if act == self.n_targets + 1 and state['fuel'] > 0: # Thrust Up
                sat.hub.v_CN_NInit = v_curr + v_dir * 20.0 # TƒÉng l·ª±c ƒë·∫©y l√™n 20m/s ƒë·ªÉ th·∫•y hi·ªáu qu·∫£ nhanh h∆°n
                state['fuel'] -= 1.0
                rewards[i] += thrust_penalty
                
            elif act == self.n_targets + 2 and state['fuel'] > 0: # Thrust Down
                sat.hub.v_CN_NInit = v_curr - v_dir * 20.0
                state['fuel'] -= 1.0
                rewards[i] += thrust_penalty

            # --- CHI·∫æN L∆Ø·ª¢C 2: R√ÄNG BU·ªòC ƒê·ªò CAO (QUAN TR·ªåNG) ---
            # Gi·∫£ s·ª≠:
            # - M·ª•c ti√™u s·ªë ch·∫µn (0, 2, 4...): C·∫ßn ƒë·ªô ph√¢n gi·∫£i cao -> Ph·∫£i bay TH·∫§P (< 500km)
            # - M·ª•c ti√™u s·ªë l·∫ª (1, 3, 5...): C·∫ßn v√πng ph·ªß r·ªông -> Ph·∫£i bay CAO (> 800km)
            
            if 0 <= act < self.n_targets:
                # ... (Code t√≠nh to√°n g√≥c quay gi·ªØ nguy√™n) ...
                tgt_pos = self.targets_ecef[act]
                req_vec = tgt_pos - r_curr_vec
                dist = np.linalg.norm(req_vec)
                req_vec /= (dist + 1e-9)
                
                cos_theta = np.dot(state['bore_vec'], req_vec)
                angle = math.acos(np.clip(cos_theta, -1.0, 1.0))
                t_slew = angle / self.max_slew_rate

                if t_slew < self.decision_dt:
                    state['bore_vec'] = req_vec
                    
                    # ƒêI·ªÄU KI·ªÜN CH·ª§P N√ÇNG CAO:
                    is_even_target = (act % 2 == 0)
                    capture_success = False
                    
                    if is_even_target:
                        # M·ª•c ti√™u ch·∫µn: Y√™u c·∫ßu ƒë·ªô cao th·∫•p (V√≠ d·ª• < 600km)
                        if altitude < 600e3 and dist < 3000e3:
                            rewards[i] += 15.0 # Th∆∞·ªüng ƒë·∫≠m
                            capture_success = True
                        else:
                            # N·∫øu nh√¨n th·∫•y nh∆∞ng sai ƒë·ªô cao -> Ph·∫°t nh·∫π ƒë·ªÉ nh·∫Øc nh·ªü
                            rewards[i] -= 0.1 
                    else:
                        # M·ª•c ti√™u l·∫ª: Y√™u c·∫ßu ƒë·ªô cao cao (V√≠ d·ª• > 800km)
                        if altitude > 800e3 and dist < 4000e3:
                            rewards[i] += 15.0
                            capture_success = True
                        else:
                            rewards[i] -= 0.1

                    if capture_success:
                        state['captured_targets'].add(act)
                else:
                    rewards[i] -= 0.1

        # 2. Ch·∫°y m√¥ ph·ªèng v·∫≠t l√Ω
        stop_time = self.sim_time + self.decision_dt
        self.scSim.ConfigureStopTime(int(macros.sec2nano(stop_time)))
        self.scSim.ExecuteSimulation()
        self.sim_time = stop_time
        
        # 3. Check done
        done = False
        if self.sim_time > 5400: done = True
        
        next_obs = self._get_all_obs()
        return next_obs, rewards, [done]*self.n_sats, {}
    '''

    def step(self, actions, debug_mode=True):
            """
            debug_mode=True: S·∫Ω in th√¥ng b√°o h√†nh ƒë·ªông ra m√†n h√¨nh console.
            """
            # actions: list g·ªìm n_sats h√†nh ƒë·ªông
            rewards = np.zeros(self.n_sats)
            dones = [False] * self.n_sats
            
            # 1. X·ª≠ l√Ω h√†nh ƒë·ªông cho t·ª´ng v·ªá tinh
            for i, action in enumerate(actions):
                sat = self.sats[i]
                state = self.states[i]
                act = int(action)
                
                # L·∫•y th√¥ng tin v·∫≠t l√Ω hi·ªán t·∫°i
                v_curr = np.array(sat.hub.v_CN_NInit).flatten()
                v_mag = np.linalg.norm(v_curr)
                v_dir = v_curr / (v_mag + 1e-9)
                
                r_curr = np.array(sat.hub.r_CN_NInit).flatten()
                alt_km = (np.linalg.norm(r_curr) - self.earth_radius) / 1000.0
                
                # --- KI·ªÇM TRA H√ÄNH ƒê·ªòNG & LOGGING ---
                
                # A. H√ÄNH ƒê·ªòNG: TƒÇNG T·ªêC / N√ÇNG QU·ª∏ ƒê·∫†O
                if act == self.n_targets + 1: 
                    if state['fuel'] > 0:
                        # V·∫≠t l√Ω: TƒÉng v·∫≠n t·ªëc -> Orbit to ra -> ƒê·ªô cao tƒÉng
                        sat.hub.v_CN_NInit = v_curr + v_dir * 20.0 # Delta-V = 20 m/s
                        state['fuel'] -= 1.0
                        rewards[i] -= 0.05 # Ph·∫°t nh·∫π ti·ªÅn xƒÉng
                        
                        if debug_mode:
                            print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | üî• THRUST UP  (H: {alt_km:.1f}km) -> ƒêang N√¢ng Qu·ªπ ƒê·∫°o")
                    else:
                        if debug_mode: print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | ‚ö†Ô∏è H·∫æT NHI√äN LI·ªÜU (Kh√¥ng th·ªÉ Thrust Up)")

                # B. H√ÄNH ƒê·ªòNG: GI·∫¢M T·ªêC / H·∫† QU·ª∏ ƒê·∫†O
                elif act == self.n_targets + 2: 
                    if state['fuel'] > 0:
                        # V·∫≠t l√Ω: Gi·∫£m v·∫≠n t·ªëc -> Orbit co l·∫°i -> ƒê·ªô cao gi·∫£m
                        sat.hub.v_CN_NInit = v_curr - v_dir * 20.0
                        state['fuel'] -= 1.0
                        rewards[i] -= 0.05
                        
                        if debug_mode:
                            print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | üîª THRUST DOWN (H: {alt_km:.1f}km) -> ƒêang H·∫° ƒê·ªô Cao")
                    else:
                        if debug_mode: print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | ‚ö†Ô∏è H·∫æT NHI√äN LI·ªÜU (Kh√¥ng th·ªÉ Thrust Down)")

                # C. H√ÄNH ƒê·ªòNG: XOAY B√ÅNH ƒê√Ä (SLEW) & CH·ª§P ·∫¢NH
                elif 0 <= act < self.n_targets:
                    tgt_pos = self.targets_ecef[act]
                    
                    # T√≠nh to√°n g√≥c quay
                    req_vec = tgt_pos - r_curr
                    dist = np.linalg.norm(req_vec)
                    req_vec /= (dist + 1e-9)
                    
                    cos_theta = np.dot(state['bore_vec'], req_vec)
                    angle_deg = math.degrees(math.acos(np.clip(cos_theta, -1.0, 1.0)))
                    
                    # Th·ªùi gian c·∫ßn ƒë·ªÉ xoay
                    t_slew = math.radians(angle_deg) / self.max_slew_rate
                    
                    if t_slew < self.decision_dt:
                        # Xoay k·ªãp!
                        state['bore_vec'] = req_vec 
                        
                        # Logic ph·∫ßn th∆∞·ªüng (nh∆∞ c≈©)
                        is_even_target = (act % 2 == 0)
                        capture_success = False
                        
                        if is_even_target and alt_km < 600 and dist < 3000e3:
                            rewards[i] += 15.0
                            capture_success = True
                        elif not is_even_target and alt_km > 800 and dist < 4000e3:
                            rewards[i] += 15.0
                            capture_success = True
                        else:
                            rewards[i] -= 0.1 # Nh√¨n th·∫•y nh∆∞ng sai ƒë·ªô cao
                        
                        if debug_mode and capture_success:
                            print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | üì∏ CH·ª§P TH√ÄNH C√îNG Target {act} | G√≥c l·ªách: {angle_deg:.1f}¬∞ (ƒê√£ xoay xong)")
                        elif debug_mode and not capture_success:
                            # Log n·∫øu xoay t·ªõi nh∆∞ng ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán ch·ª•p (ƒë·ªÉ debug)
                            pass 
                            # print(f"[t={self.sim_time:.0f}s] Sat {i} xoay t·ªõi T{act} nh∆∞ng sai ƒë·ªô cao/kho·∫£ng c√°ch.")
                    else:
                        # Xoay kh√¥ng k·ªãp
                        rewards[i] -= 0.1
                        if debug_mode and angle_deg > 10.0: # Ch·ªâ log n·∫øu g√≥c l·ªách l·ªõn
                            print(f"[t={self.sim_time:.0f}s] üõ∞Ô∏è Sat {i} | üîÑ ƒêANG XOAY t·ªõi T{act} (L·ªách {angle_deg:.1f}¬∞ - C·∫ßn {t_slew:.1f}s)")

            # 2. Ch·∫°y m√¥ ph·ªèng v·∫≠t l√Ω
            stop_time = self.sim_time + self.decision_dt
            self.scSim.ConfigureStopTime(int(macros.sec2nano(stop_time)))
            self.scSim.ExecuteSimulation()
            self.sim_time = stop_time
            
            # 3. Check done
            if self.sim_time > 5400: dones = [True] * self.n_sats
            
            next_obs = self._get_all_obs()
            return next_obs, rewards, dones, {}
# --- 4. MAPPO TRAINER ---
def train_mappo():
    N_SATS = 4
    N_TARGETS = 50
    OBS_DIM = 7
    ACT_DIM = N_TARGETS + 3
    # Global State = Gh√©p obs c·ªßa t·∫•t c·∫£ v·ªá tinh l·∫°i
    GLOBAL_STATE_DIM = OBS_DIM * N_SATS 
    
    env = BasiliskMultiSatEnv(n_sats=N_SATS, n_targets=N_TARGETS, viz=True)
    
    # Chia s·∫ª tham s·ªë (Parameter Sharing): T·∫•t c·∫£ v·ªá tinh d√πng chung 1 m·∫°ng MAPPO
    mappo_agent = MultiAgentActorCritic(OBS_DIM, GLOBAL_STATE_DIM, ACT_DIM)
    optimizer = optim.Adam(mappo_agent.parameters(), lr=1e-3)
    
    print("--- B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN MAPPO (4 V·ªá tinh) ---")
    
    for epoch in range(10): # Demo 10 epochs
        obs_list = env.reset() # List [Sat1_obs, Sat2_obs, ...]
        ep_rewards = np.zeros(N_SATS)
        done = False
        
        # Buffer l∆∞u tr·ªØ
        batch_obs = []
        batch_global_state = []
        batch_actions = []
        batch_rewards = []
        
        while not done:
            # 1. Decentralized Execution
            actions_t = []
            
            # T·∫°o Global State cho Critic
            global_state_t = np.concatenate(obs_list) 
            
            for i in range(N_SATS):
                obs_tensor = torch.FloatTensor(obs_list[i]).unsqueeze(0)
                logits = mappo_agent.act(obs_tensor) # Actor d√πng obs c·ª•c b·ªô
                
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample()
                actions_t.append(action.item())
            
            # 2. M√¥i tr∆∞·ªùng ph·∫£n h·ªìi
            next_obs_list, rewards, dones, _ = env.step(actions_t)
            
            # L∆∞u v√†o buffer
            batch_obs.append(obs_list)
            batch_global_state.append(global_state_t)
            batch_actions.append(actions_t)
            batch_rewards.append(rewards)
            
            obs_list = next_obs_list
            ep_rewards += rewards
            done = all(dones)

        # 3. Centralized Training
        # T√≠nh Returns
        returns_batch = np.zeros_like(batch_rewards)
        running_add = np.zeros(N_SATS)
        for t in reversed(range(len(batch_rewards))):
            running_add = batch_rewards[t] + 0.99 * running_add
            returns_batch[t] = running_add
            
        # Update
# Update
        optimizer.zero_grad()
        loss = 0
        
        # C·∫•u h√¨nh h·ªá s·ªë (Hyperparameters)
        vf_coeff = 0.5   # H·ªá s·ªë quan tr·ªçng c·ªßa Critic (Value Loss)
        ent_coeff = 0.05 # H·ªá s·ªë Entropy (C√†ng cao c√†ng kh√°m ph√° nhi·ªÅu) <-- TƒÇNG C√ÅI N√ÄY L√äN
        
        for t in range(len(batch_obs)):
            g_state = torch.FloatTensor(batch_global_state[t]).unsqueeze(0)
            target_value = torch.FloatTensor([np.mean(returns_batch[t])]).unsqueeze(0)
            
            # --- 1. Calculate Value Loss (Critic) ---
            predicted_value = mappo_agent.evaluate(g_state)
            value_loss = nn.MSELoss()(predicted_value, target_value)
            
            # --- 2. Calculate Policy Loss & Entropy (Actor) ---
            actor_loss_sum = 0
            entropy_sum = 0 # Bi·∫øn ƒë·ªÉ c·ªông d·ªìn entropy c·ªßa 4 v·ªá tinh
            
            for i in range(N_SATS):
                obs = torch.FloatTensor(batch_obs[t][i]).unsqueeze(0)
                act = torch.tensor([batch_actions[t][i]])
                ret = returns_batch[t][i]
                
                advantage = ret - predicted_value.item()
                
                logits = mappo_agent.act(obs)
                dist = torch.distributions.Categorical(logits=logits)
                log_prob = dist.log_prob(act)
                
                # [M·ªöI] T√≠nh Entropy: ƒêo ƒë·ªô "ng·∫´u nhi√™n/b·ªëi r·ªëi" c·ªßa Actor
                entropy_sum += dist.entropy() 
                
                actor_loss_sum += -log_prob * advantage
            
            # T√≠nh trung b√¨nh Entropy c·ªßa c·∫£ ƒë·ªôi t·∫°i b∆∞·ªõc th·ªùi gian t
            avg_entropy = entropy_sum / N_SATS

            # --- [ƒêO·∫†N B·∫†N C·∫¶N TH√äM V√ÄO ƒê√ÇY] ---
            # C√¥ng th·ª©c: Loss = Actor_Loss + (0.5 * Critic_Loss) - (0.05 * Entropy)
            # D·∫•u tr·ª´ (-) tr∆∞·ªõc Entropy nghƒ©a l√†: Entropy c√†ng cao -> Loss c√†ng th·∫•p -> T·ªët
            step_loss = actor_loss_sum + (vf_coeff * value_loss) - (ent_coeff * avg_entropy)
            
            loss += step_loss
            
        loss.backward()
        # Th√™m clipping gradient ƒë·ªÉ tr√°nh update qu√° m·∫°nh l√†m h·ªèng m·∫°ng
        nn.utils.clip_grad_norm_(mappo_agent.parameters(), max_norm=0.5) 
        optimizer.step()
        
        avg_r = np.mean(ep_rewards)
        print(f"Epoch {epoch+1} | Avg Team Reward: {avg_r:.1f}")
        
# --- [S·ª¨A 2] TH√äM ƒêO·∫†N DEMO N√ÄY V√ÄO CU·ªêI ---
    print("\n--- CH·∫†Y DEMO KI·ªÇM TRA ---")
    obs_list = env.reset()
    done = False
    while not done:
        actions_t = []
        for i in range(N_SATS):
            obs_tensor = torch.FloatTensor(obs_list[i]).unsqueeze(0)
            logits = mappo_agent.act(obs_tensor)
            action = torch.argmax(logits, dim=1) # Ch·ªçn h√†nh ƒë·ªông t·ªët nh·∫•t
            actions_t.append(action.item())
            
        # B·∫¨T DEBUG ·ªû ƒê√ÇY
        next_obs_list, rewards, dones, _ = env.step(actions_t, debug_mode=True)
        obs_list = next_obs_list
        done = all(dones)

    print("--- XONG ---")
    print("M·ªü file 'mappo_constellation_new.bin' trong Vizard ƒë·ªÉ xem 4 v·ªá tinh ph·ªëi h·ª£p.")

if __name__ == "__main__":
    train_mappo()